{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "#hllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.1\\\\bin\\\\cudart64_101.dll\")\n",
    "#incase cudart64_101.dll is unable to be loaded\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from medpy.io import load\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras import Input, Model, models, layers\n",
    "import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import sklearn\n",
    "from scipy.ndimage import rotate\n",
    "import scipy\n",
    "import time\n",
    "from sklearn import datasets, preprocessing, model_selection, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import graphviz\n",
    "import pydotplus\n",
    "%matplotlib inline\n",
    "\n",
    "#logging.basicConfig(level = 'logging.info')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7824564768173545159\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16500547467963836693\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3048629862\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13502513082923641504\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6757887804049758187\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "'''Confirming running on GPU'''\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "'''\n",
    "CUDA_VISIBLE_DEVICES = 0,1\n",
    "'''\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predetermined Functions'''\n",
    "def Generic_image_generator(URL):\n",
    "    image_data, image_header = load(URL)\n",
    "    for iter in range(0,len(image_data[:][:])):\n",
    "        yield image_data[iter]\n",
    "\n",
    "def create_ID_dict(basepath): #currently only set to return CT and segmentation mask images, not CTA's\n",
    "    '''basepath is location of file that is holding dataset, datasets should be deeper in the file tree'''\n",
    "    logging.info(\"creating dataset location directory/dictionary from {}\".format(basepath))\n",
    "    dict = {}\n",
    "    mask_list = []\n",
    "    image_list = []\n",
    "    for root, dirs, list_of_files in os.walk(basepath):\n",
    "        for files in list_of_files:\n",
    "            if \"r.mhd\" in files: #checks if files ends in r.mhd, signifying a segmentation truth. appeneded to masked list\n",
    "                mask_list.append(os.path.join(root,files))\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "            if \"cti.mhd\" in files: #checks if files end in CTI.mhd, and appends it to image list\n",
    "                image_list.append(os.path.join(root,files))\n",
    "            else:\n",
    "                pass\n",
    "    #for the current moment CTA images are not being uploaded.\n",
    "            \n",
    "    dict.update({\"image\" : image_list })\n",
    "    dict.update({\"mask\" : mask_list })\n",
    "    logging.info(\"dictionary created\")\n",
    "    \n",
    "    return dict\n",
    "\n",
    "def image_preprocessing(array):\n",
    "    logging.info(\"Beginning Preprocessing\")\n",
    "    \n",
    "    logging.info(\"Rotating Image into axial plane\")\n",
    "    array = rotate(array, 90, axes=(0,2), reshape=True) #rotates to axial form sup to inf direction.\n",
    "    \n",
    "    logging.info(\"Making image to uniform size\")\n",
    "    \n",
    "    blank_array = np.full((1,512,512),-3024) #-3024 is the \"background\" of the images, and is considered the value for non truth in the segmentation masks\"\n",
    "    \n",
    "    while len(array) < 69: #this while loop with following if statement helps bring all images to size 70x512x512\n",
    "        array = np.append(array, blank_array, axis = 0)\n",
    "        array = np.append(blank_array, array, axis = 0)\n",
    "        \n",
    "    if len(array) == 69:\n",
    "        array = np.append(array,blank_array, axis = 0)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    logging.info(\"Image Preprocessing is complete\")\n",
    "    \n",
    "    return array\n",
    "    \n",
    "            \n",
    "def image_generator(img_dict, mask):\n",
    "    \n",
    "    include_mask = False\n",
    "    \n",
    "    logging.info(\"creating generator for {}.\".format(img_dict))\n",
    "    \n",
    "    if mask == True: #checking if dictoinary contains assoicated mask images.\n",
    "        include_mask = True\n",
    "    \n",
    "    if include_mask == True:\n",
    "        \n",
    "        for path1, path2 in zip(img_dict['image'],img_dict['mask']):\n",
    "            logging.info(\"loading {} {} into generator\".format(\"image\",os.path.basename(path1)))\n",
    "            image, header_img = load(path1) #load simage, and gets image header. Currently image header is not being exported from generator.\n",
    "            print(\"Image processing\")\n",
    "            image = image_preprocessing(image)\n",
    "            \n",
    "            \n",
    "            logging.info(\"loading {} {} into generator\".format(\"mask\",os.path.basename(path2)))\n",
    "            mask_img, header_mask = load(path2) #load mask, and gets mask image header. Currently mask header is not being exported from generator.\n",
    "            print(\"Mask Processing\")\n",
    "            mask_img = image_preprocessing(mask_img)\n",
    "        \n",
    "            logging.info(\"Generator is prepped\")\n",
    "            \n",
    "            for img_slice, mask_slice in zip(image,mask_img): #This for loop is to make image the correct tensor length for TF.\n",
    "                img_slice = np.reshape(img_slice,(1,1,512,512))\n",
    "                mask_slice = np.reshape(mask_slice,(1,1,512,512))\n",
    "                \n",
    "                yield img_slice, mask_slice\n",
    "            \n",
    "    elif include_mask == False:\n",
    "        for path in dict_obj[\"image\"]: #load simage, and gets image header. Currently image header is not being exported from generator.\n",
    "            logging.info(\"loading {} {} into generator\".format(\"image\",os.path.basename(path)))\n",
    "            image, header  = load(path)\n",
    "            image = image_preprocessing(image)\n",
    "            logging.info(\"Generator is prepped\")\n",
    "            \n",
    "            for slice in image:  #This for loop is to make image the correct tensor length for TF.\n",
    "                slice = np.reshape(slice,(1,1,512,512))\n",
    "                yield slice\n",
    "    else:\n",
    "        raise TypeError(\"Problem with inputs.\")\n",
    "        logging.error(\"Unable to load image into generator\")\n",
    "            \n",
    "    #no normalization has been added yet. Will need to think about how I want to approach this.\n",
    "    #from skimage.color import rgb2gray to make things grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Predetermined Variables'''\n",
    "PATH_tr = 'C:\\\\Users\\\\Andrew_Arbogast\\\\Desktop\\\\Codes\\\\UCAIDM\\\\CAIDM Project\\\\DataSets\\\\orCAScore Dataset\\\\Filtered\\\\Training_Set\\\\Train\\\\'\n",
    "PATH_val = 'C:\\\\Users\\\\Andrew_Arbogast\\\\Desktop\\\\Codes\\\\UCAIDM\\\\CAIDM Project\\\\DataSets\\\\orCAScore Dataset\\\\Filtered\\\\Training_Set\\\\Validation\\\\'\n",
    "PATH_te = 'C:\\\\Users\\\\Andrew_Arbogast\\\\Desktop\\\\Codes\\\\UCAIDM\\\\CAIDM Project\\\\DataSets\\\\orCAScore Dataset\\\\Filtered\\\\Testing_Set\\\\'\n",
    "Model_path = 'C:\\\\Users\\\\Andrew_Arbogast\\\\Desktop\\\\Codes\\\\UCAIDM\\\\CAIDM Project\\\\Coronary_Calcium\\\\Models\\\\'\n",
    "model_vers = 'First_Attempt'\n",
    "input_shape = (1,512,512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Creation Took 0.025994539260864258 seconds\n",
      "Generators Creation Took 0.0 sec\n"
     ]
    }
   ],
   "source": [
    "''' Loading Data Sets '''\n",
    "logging.info(\"loading datasets\")\n",
    "\n",
    "start_time = time.time()\n",
    "tr_dict = create_ID_dict(PATH_tr)\n",
    "val_dict = create_ID_dict(PATH_val)\n",
    "te_dict = create_ID_dict(PATH_te)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Dictionary Creation Took {} seconds\".format(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "tr_data = image_generator(tr_dict,mask = True)\n",
    "val_data = image_generator(val_dict, mask = True)\n",
    "te_data = image_generator(te_dict, mask = False)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "print(\"Generators Creation Took {} sec\".format(end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating NN'''\n",
    "\n",
    "\n",
    "if os.path.isdir(Model_path+model_vers)==True:\n",
    "    logging.info(\"Loading Network\")\n",
    "    model=tfk.models.load_model(\"Model_path+model_vers\")\n",
    "    \n",
    "else:\n",
    "    logging.info(\"Creating Network\")\n",
    "#https://towardsdatascience.com/step-by-step-implementation-3d-convolutional-neural-network-in-keras-12efbdd7b130    \n",
    "    model= tfk.Sequential()\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape, data_format=\"channels_first\", padding='SAME'))\n",
    "    #model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "    #model.add(layers.BatchNormalization(center=True, scale=True))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "    #model.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', padding ='SAME'))\n",
    "    #model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
    "    #model.add(layers.BatchNormalization(center=True, scale=True))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "    #model.add(layers.Flatten())\n",
    "    #model.add(layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    #model.add(layers.Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "    #model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "\n",
    "'''Hyper Parameters'''\n",
    "logging.info(\"Adding Hyperparameters\")\n",
    "\n",
    "optimizer=tfk.optimizers.Adadelta() #'Adam' \n",
    "loss= \"categorical_crossentropy\" #'sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "model.compile(optimizer = optimizer, loss= loss, metrics= metrics )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image processing\n",
      "Mask Processing\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-7-4e9f20ab5861>:2) ]] [Op:__inference_predict_function_134]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4e9f20ab5861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    844\u001b[0m               *args, **kwds)\n\u001b[0;32m    845\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\andrew_arbogast\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-7-4e9f20ab5861>:2) ]] [Op:__inference_predict_function_134]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "test_image, test_mask = next(tr_data)\n",
    "model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''n=0\n",
    "while n<25:\n",
    "    for img_slice, mask_slice in tr_data:\n",
    "        print(\"Image shape is: \",img_slice.shape)\n",
    "        print(\"Mask shape is :\", mask_slice.shape)\n",
    "        n+=1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training NN\"\"\"\n",
    "logging.info(\"Training Neural Network\")\n",
    "#tf.autograph.experimental.do_not_convert(func=model.fit(tr_data,epochs = 10,verbose=10))\n",
    "model.fit(tr_data,epochs = 10,verbose=10)\n",
    "#model.save(\"Model_path+model_vers\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
